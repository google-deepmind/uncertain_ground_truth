{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGU3SbqlTjIY"
      },
      "source": [
        "# Toy dataset and models\n",
        "\n",
        "See `README.md` for installation and usage instructions.\n",
        "\n",
        "This notebook creates the reference toy dataset used throughout [1], starting\n",
        "with Figure 3. However, it also includes pseudo annotations following the format\n",
        "in our skin condition case study, namely partial rankings, as discussed in [2].\n",
        "\n",
        "```\n",
        "[1] Stutz, D., Roy, A.G., Matejovicova, T., Strachan, P., Cemgil, A.T.,\n",
        "    \u0026 Doucet, A. (2023).\n",
        "    Conformal prediction under ambiguous ground truth. ArXiv, abs/2307.09302.\n",
        "[2] Stutz, D., Cemgil, A.T., Roy, A.G., Matejovicova, T., Barsbey, M.,\n",
        "    Strachan, P., Schaekermann, M., Freyberg, J.V., Rikhye, R.V., Freeman, B.,\n",
        "    Matos, J.P., Telang, U., Webster, D.R., Liu, Y., Corrado, G.S., Matias, Y.,\n",
        "    Kohli, P., Liu, Y., Doucet, A., \u0026 Karthikesalingam, A. (2023).\n",
        "    Evaluating AI systems under uncertain ground truth: a case study in\n",
        "    dermatology. ArXiv, abs/2307.02191.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXTC5XMoTjIm"
      },
      "source": [
        "## Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfnBXIuYTjIo"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import sklearn.neural_network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj0Z7TMjTjIt"
      },
      "outputs": [],
      "source": [
        "import formats\n",
        "import irn as aggregation\n",
        "import gaussian_toy_dataset as gtd\n",
        "import colab_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkCN5gGATjIu"
      },
      "outputs": [],
      "source": [
        "colab_utils.set_style()\n",
        "plot_hist = colab_utils.plot_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5urKR2GTjIw"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AKrJIQ0TjIy"
      },
      "source": [
        "The Gaussian toy dataset samples examples from multiple overlapping Gaussians, see `gaussian_toy_dataset.py` for details.\n",
        "\n",
        "Here, we create the 2-dimensional reference examples used in [1] for illustrative purposes and an easy way to play around with this repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X89A4yhTjI0"
      },
      "outputs": [],
      "source": [
        "def get_data(config):\n",
        "  \"\"\"Generate data using the config.\"\"\"\n",
        "  # Defines a dataset of multiple overlapping Gaussians.\n",
        "  generator = gtd.GaussianToyDataset(\n",
        "      config['rng'], jnp.array(config['class_weights']),\n",
        "      config['class_sigmas'], config['dimensionality'], config['sigma'])\n",
        "  num_examples = config['train_examples'] + config['test_examples']\n",
        "  # Sample points x from the overlapping Gaussian distributions.\n",
        "  examples, ground_truths = generator.sample_points(num_examples)\n",
        "  # Compute the true posterior distributions p(y|x).\n",
        "  human_ground_truths = generator.evaluate_points(examples)\n",
        "  # Sample annotator rankings for all points.\n",
        "  rankings, groups = generator.sample_rankings(\n",
        "      human_ground_truths,\n",
        "      config['reader_sharpness'],\n",
        "      config['expected_length'],\n",
        "      config['grouping_threshold'])\n",
        "  # Convert rankings and compute IRN aggregation.\n",
        "  selectors = formats.convert_rankings_to_selectors(rankings, groups)\n",
        "  irn = aggregation.aggregate_irn(rankings, groups)\n",
        "  return {\n",
        "      'config': config,\n",
        "      'train_examples': examples[:config['train_examples']],\n",
        "      'train_labels': ground_truths[:config['train_examples']],\n",
        "      'train_smooth_labels': human_ground_truths[:config['train_examples']],\n",
        "      'train_rankings': rankings[:config['train_examples']],\n",
        "      'train_groups': groups[:config['train_examples']],\n",
        "      'train_selectors': selectors[:config['train_examples']],\n",
        "      'train_irn': irn[:config['train_examples']],\n",
        "      'test_examples': examples[config['train_examples']:],\n",
        "      'test_labels': ground_truths[config['train_examples']:],\n",
        "      'test_smooth_labels': human_ground_truths[config['train_examples']:],\n",
        "      'test_rankings': rankings[config['train_examples']:],\n",
        "      'test_groups': groups[config['train_examples']:],\n",
        "      'test_selectors': selectors[config['train_examples']:],\n",
        "      'test_irn': irn[config['train_examples']:],\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1pbICb8TjI2"
      },
      "outputs": [],
      "source": [
        "config = {}\n",
        "config['rng'] = gtd.PRNGSequence(5)\n",
        "config['dimensionality'] = 2\n",
        "config['sigma'] = 0.3\n",
        "config['class_weights'] = [1]*3\n",
        "config['class_sigmas'] = 0.1\n",
        "config['train_examples'] = 1000\n",
        "# Note that in the paper we used 20000 test examples.\n",
        "config['test_examples'] = 1000\n",
        "config['expected_length'] = 1.5\n",
        "config['grouping_threshold'] = 0.05\n",
        "# Number of readers and their sharpness.\n",
        "config['reader_sharpness'] = jnp.array([500000, 100000, 50000, 1000000, 500000, 150000, 100000, 1000000, 100000, 90000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W_Wou0pTjI4"
      },
      "outputs": [],
      "source": [
        "data = get_data(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkTYwKMcTjI6"
      },
      "outputs": [],
      "source": [
        "colors = np.array([\n",
        "    [228,26,28],\n",
        "    [55,126,184],\n",
        "    [77,175,74],\n",
        "]) / 255.\n",
        "colab_utils.plot_data(\n",
        "    data['train_examples'], data['train_labels'],\n",
        "    title='Examples with their true labels', name='data', colors=colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHv4ANLSTjI7"
      },
      "outputs": [],
      "source": [
        "colab_utils.plot_data(\n",
        "    data['train_examples'],\n",
        "    np.argmax(data['train_smooth_labels'], axis=1),\n",
        "    title='Examples with their voted labels', name='data_top1', colors=colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tsqfrlVTjI8"
      },
      "outputs": [],
      "source": [
        "colab_utils.plot_smooth_data(\n",
        "      data['train_examples'], data['train_smooth_labels'], name='data_smooth', colors=colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBmy8PumTjI9"
      },
      "outputs": [],
      "source": [
        "with open('data/toy_data.pkl', 'wb') as f:\n",
        "  pickle.dump(data, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNTxLFOBTjI9"
      },
      "source": [
        "## Model\n",
        "\n",
        "We train a small MLP. Note that in the paper, we trained our own 2-layer MLP using Haiku; for simplicty this Colab uses `sklearn` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktgwvs8xTjI-"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "for seed in range(4):\n",
        "  classifier = sklearn.neural_network.MLPClassifier(alpha=1, max_iter=(seed + 1) * 25, random_state=seed)\n",
        "  classifier.fit(\n",
        "      data['train_examples'],\n",
        "      jax.nn.one_hot(data['train_labels'], 3))\n",
        "  predictions_k = classifier.predict_log_proba(data['test_examples'])\n",
        "  predictions_k = jax.nn.softmax(predictions_k)\n",
        "  predictions.append(predictions_k)\n",
        "predictions = jnp.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPJOQM-hTjI-"
      },
      "outputs": [],
      "source": [
        "for seed in range(predictions.shape[0]):\n",
        "    print(seed, jnp.mean(data['test_labels'] == jnp.argmax(predictions[seed], axis=1)))\n",
        "    with open(f'data/toy_predictions{seed}.pkl', 'wb') as f:\n",
        "        pickle.dump(predictions[seed], f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
