{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y3zf-xKT_xG"
      },
      "source": [
        "# Toy dataset: Monte Carlo conformal prediction\n",
        "\n",
        "See `README.md` for installation and usage instructions.\n",
        "\n",
        "This notebook re-creates some of the examples and figures from [1] on the toy\n",
        "dataset.\n",
        "\n",
        "```\n",
        "[1] Stutz, D., Roy, A.G., Matejovicova, T., Strachan, P., Cemgil, A.T.,\n",
        "    \u0026 Doucet, A. (2023).\n",
        "    Conformal prediction under ambiguous ground truth. ArXiv, abs/2307.09302.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spNS_87xT_xW"
      },
      "source": [
        "## Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uct1UkQYT_xY"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPPlUEudT_xc"
      },
      "outputs": [],
      "source": [
        "import conformal_prediction\n",
        "import monte_carlo\n",
        "import p_value_combination\n",
        "import plausibility_regions\n",
        "import classification_metrics\n",
        "import colab_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RuPqQ0FT_xd"
      },
      "outputs": [],
      "source": [
        "colab_utils.set_style()\n",
        "plot_hist = colab_utils.plot_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msdEDh4BT_xe"
      },
      "outputs": [],
      "source": [
        "with open('toy_data.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U86LZsIVT_xg"
      },
      "outputs": [],
      "source": [
        "with open('toy_predictions0.pkl', 'rb') as f:\n",
        "  predictions = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fWLDNwyT_xi"
      },
      "source": [
        "## CP with true and top-1 labels\n",
        "\n",
        "Compare standard conformal prediction calibrated against majority voted (top-1) labels and conformal prediction calibrated against the true labels -- which we have access to on the toy dataset but not in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmuS4FaqT_xj"
      },
      "outputs": [],
      "source": [
        "def run_trial(\n",
        "    predictions, data, method='',\n",
        "    alpha=0.1, trials=100, split=0.5):\n",
        "  \"\"\"Run a conformal prediction experiment.\"\"\"\n",
        "  results = {}\n",
        "  tags = ['inefficiencies', 'true_coverages', 'aggregated_coverages', 'top1_coverages']\n",
        "  for tag in tags:\n",
        "    results[tag] = []\n",
        "  keys = jax.random.split(jax.random.PRNGKey(0), 2 * trials)\n",
        "  for t in range(trials):\n",
        "    permutation = jax.random.permutation(keys[2 * t], predictions.shape[0])\n",
        "    val_examples = int(predictions.shape[0]*split)\n",
        "    val_predictions = predictions[permutation[:val_examples]]\n",
        "    test_predictions = predictions[permutation[val_examples:]]\n",
        "    val_human_ground_truth = data['test_smooth_labels'][permutation[:val_examples]]\n",
        "    test_human_ground_truth = data['test_smooth_labels'][permutation[val_examples:]]\n",
        "    val_ground_truth = data['test_labels'][permutation[:val_examples]]\n",
        "    test_ground_truth = data['test_labels'][permutation[val_examples:]]\n",
        "\n",
        "    if method == 'top1':\n",
        "      val_labels = jnp.argmax(val_human_ground_truth, axis=1)\n",
        "      threshold = conformal_prediction.calibrate_threshold(\n",
        "          val_predictions, val_labels, alpha)\n",
        "    elif method == 'mccp':\n",
        "      threshold = monte_carlo.calibrate_mc_threshold(\n",
        "          keys[2 * t + 1], val_predictions, val_human_ground_truth, num_samples=10, alpha=alpha)\n",
        "    else:\n",
        "      val_labels = val_ground_truth\n",
        "      threshold = conformal_prediction.calibrate_threshold(\n",
        "          val_predictions, val_labels, alpha)\n",
        "\n",
        "    confidence_sets = conformal_prediction.predict_threshold(\n",
        "        test_predictions, threshold)\n",
        "\n",
        "    results['inefficiencies'].append(classification_metrics.size(\n",
        "        confidence_sets))\n",
        "    test_one_hot_ground_truth = jax.nn.one_hot(\n",
        "        test_ground_truth, confidence_sets.shape[1])\n",
        "    results['true_coverages'].append(classification_metrics.aggregated_coverage(\n",
        "        confidence_sets, test_one_hot_ground_truth))\n",
        "    test_top1_ground_truth = jax.nn.one_hot(\n",
        "        jnp.argmax(test_human_ground_truth, axis=1),\n",
        "        test_human_ground_truth.shape[1])\n",
        "    results['top1_coverages'].append(\n",
        "        classification_metrics.aggregated_coverage(\n",
        "            confidence_sets, test_top1_ground_truth))\n",
        "    results['aggregated_coverages'].append(\n",
        "        classification_metrics.aggregated_coverage(\n",
        "            confidence_sets, test_human_ground_truth))\n",
        "  for tag in tags:\n",
        "    results[tag] = jnp.array(results[tag])\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebWU-rxcT_xp"
      },
      "outputs": [],
      "source": [
        "def plot_coverage_top1_calibration(predictions, data, **kwargs):\n",
        "  \"\"\"Plot coverage when calibrating against true or top-1 labels.\"\"\"\n",
        "  top1_results = run_trial(\n",
        "      predictions, data, method='top1',\n",
        "      alpha=0.05, trials=100)\n",
        "  plot_hist(jnp.mean(top1_results['true_coverages'], axis=-1), normalize=True, label='Coverage of true labels')\n",
        "  plot_hist(jnp.mean(top1_results['top1_coverages'], axis=-1), normalize=True, label='Coverage of voted labels')\n",
        "  plt.vlines(0.95, 0, 0.15, color='black', label='Target')\n",
        "  plt.title('Calibration with voted labels')\n",
        "  plt.xlabel('Empirical coverage')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend()\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  if kwargs.get('name', False):\n",
        "      plt.savefig(kwargs.get('name') + '.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsWg5WiCT_xq"
      },
      "outputs": [],
      "source": [
        "plot_coverage_top1_calibration(\n",
        "    predictions, data, name='coverage_top1_calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cBMsRcCT_xr"
      },
      "outputs": [],
      "source": [
        "def plot_coverage_mccp_calibration(predictions, data, **kwargs):\n",
        "  \"\"\"Plot coverage when calibrating against true or top-1 labels.\"\"\"\n",
        "  top1_results = run_trial(\n",
        "      predictions, data, method='mccp',\n",
        "      alpha=0.05, trials=100)\n",
        "  plot_hist(jnp.mean(top1_results['true_coverages'], axis=-1), normalize=True, label='Coverage of true labels')\n",
        "  plot_hist(jnp.mean(top1_results['top1_coverages'], axis=-1), normalize=True, label='Coverage of voted labels')\n",
        "  plt.vlines(0.95, 0, 0.15, color='black', label='Target')\n",
        "  plt.title('Monte Carlo conformal calibration')\n",
        "  plt.xlabel('Empirical coverage')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend()\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  if kwargs.get('name', False):\n",
        "      plt.savefig(kwargs.get('name') + '.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aaKeilfT_xs"
      },
      "outputs": [],
      "source": [
        "plot_coverage_mccp_calibration(\n",
        "    predictions, data, name='coverage_mccp_calibration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKvzLyX1T_xs"
      },
      "source": [
        "## Aggregated coverage\n",
        "\n",
        "This experiment illustrates aggregated coverage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqh5VCeMT_xs"
      },
      "outputs": [],
      "source": [
        "def plot_aggregated_coverage(predictions, data, **kwargs):\n",
        "  true_results = run_trial(\n",
        "      predictions, data, method='',\n",
        "      alpha=0.05, trials=500)\n",
        "\n",
        "  plot_hist(\n",
        "      jnp.mean(true_results['true_coverages'], axis=1), bins=40,\n",
        "      normalize=True, label='True coverage', alpha=0.8)\n",
        "  plot_hist(\n",
        "      jnp.mean(true_results['aggregated_coverages'], axis=1), bins=40,\n",
        "      normalize=True, label='Aggregated coverage', alpha=0.8)\n",
        "  plt.title('Average coverage across trials')\n",
        "  plt.xlabel('Empirical coverage')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.xticks([0.94, 0.945, 0.95, 0.955, 0.96])\n",
        "  plt.xlim(0.94, 0.96)\n",
        "  plt.legend()\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  plt.savefig('aggregated_coverage_histogram1.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(\n",
        "      jnp.arange(true_results['true_coverages'][0].shape[0]),\n",
        "      jnp.sort(true_results['true_coverages'][0]),\n",
        "      label='True label covered')\n",
        "  plt.fill_between(\n",
        "      jnp.arange(true_results['true_coverages'][0].shape[0]),\n",
        "      jnp.zeros(true_results['true_coverages'][0].shape[0]),\n",
        "      jnp.sort(true_results['true_coverages'][0]),\n",
        "      alpha=0.2, color=plt.rcParams['axes.prop_cycle'].by_key()['color'][0],\n",
        "      label=(\n",
        "          'Realized coverage'\n",
        "          f'({jnp.mean(true_results[\"true_coverages\"][0]):.2f})'))\n",
        "  plt.plot(\n",
        "      jnp.arange(true_results['aggregated_coverages'][0].shape[0]),\n",
        "      jnp.sort(true_results['aggregated_coverages'][0]),\n",
        "      label='Plausibility mass covered')\n",
        "  plt.fill_between(\n",
        "      jnp.arange(true_results['aggregated_coverages'][0].shape[0]),\n",
        "      jnp.zeros(true_results['aggregated_coverages'][0].shape[0]),\n",
        "      jnp.sort(true_results['aggregated_coverages'][0]),\n",
        "      alpha=0.2, color=plt.rcParams['axes.prop_cycle'].by_key()['color'][1],\n",
        "      label=(\n",
        "          'Realized aggregated coverage'\n",
        "          f'({jnp.mean(true_results[\"aggregated_coverages\"][0]):.2f})'))\n",
        "  plt.title('Correctness across examples for single trial')\n",
        "  plt.xlabel('Sorted examples (separate sorting)')\n",
        "  plt.ylabel('Realized coverage')\n",
        "  plt.xlim(0, true_results['aggregated_coverages'][0].shape[0])\n",
        "  plt.legend()\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  plt.savefig('aggregated_coverage_sorted.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5NGXsBJT_xt"
      },
      "outputs": [],
      "source": [
        "plot_aggregated_coverage(predictions, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIj6snZFT_xt"
      },
      "source": [
        "## Monte Carlo conformal prediction\n",
        "\n",
        "Running and evaluation Monte Carlo conformal prediction in comparison to calibrating against the majority-voted (top-1) labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BswmRUq7T_xu"
      },
      "outputs": [],
      "source": [
        "def run_trial(\n",
        "    predictions, data,\n",
        "    alpha=0.05, num_trials=1000, split=0.5, seed=0):\n",
        "  \"\"\"Run a conformal prediction experiment.\"\"\"\n",
        "  results = {}\n",
        "  metrics = ['true_coverages', 'aggregated_coverages']\n",
        "  methods = ['top1_', 'mc_']\n",
        "  for method in methods:\n",
        "    for metric in metrics:\n",
        "      results[method + metric] = []\n",
        "\n",
        "  permutation = jax.random.permutation(\n",
        "      jax.random.PRNGKey(seed), predictions.shape[0])\n",
        "  val_examples = int(predictions.shape[0]*split)\n",
        "  val_predictions = predictions[permutation[:val_examples]]\n",
        "  test_predictions = predictions[permutation[val_examples:]]\n",
        "  val_human_ground_truth = data['test_smooth_labels'][permutation[:val_examples]]\n",
        "  test_human_ground_truth = data['test_smooth_labels'][permutation[val_examples:]]\n",
        "  test_ground_truth = data['test_labels'][permutation[val_examples:]]\n",
        "\n",
        "  keys = jax.random.split(jax.random.PRNGKey(seed + 1), num_trials)\n",
        "  for t in range(num_trials):\n",
        "\n",
        "    def evaluate_method(confidence_sets, key):\n",
        "      test_one_hot_ground_truth = jax.nn.one_hot(\n",
        "          test_ground_truth, confidence_sets.shape[1])\n",
        "      results[f'{key}true_coverages'].append(classification_metrics.aggregated_coverage(\n",
        "          confidence_sets, test_one_hot_ground_truth))\n",
        "      results[f'{key}aggregated_coverages'].append(\n",
        "          classification_metrics.aggregated_coverage(\n",
        "              confidence_sets, test_human_ground_truth))\n",
        "\n",
        "    val_top1_labels = jnp.argmax(val_human_ground_truth, axis=1)\n",
        "    top1_threshold = conformal_prediction.calibrate_threshold(\n",
        "        val_predictions, val_top1_labels, alpha)\n",
        "    top1_confidence_sets = conformal_prediction.predict_threshold(\n",
        "          test_predictions, top1_threshold)\n",
        "    evaluate_method(top1_confidence_sets, 'top1_')\n",
        "\n",
        "    val_mc_predictions, mc_labels = monte_carlo.sample_mc_labels(\n",
        "      keys[t], val_predictions, val_human_ground_truth, num_samples=1)\n",
        "    val_mc_predictions = val_mc_predictions.reshape(\n",
        "        -1, val_mc_predictions.shape[-1])\n",
        "    val_mc_labels = mc_labels.reshape(-1)\n",
        "    mc_threshold = conformal_prediction.calibrate_threshold(\n",
        "        val_mc_predictions, val_mc_labels, alpha)\n",
        "    mc_confidence_sets = conformal_prediction.predict_threshold(\n",
        "        test_predictions, mc_threshold)\n",
        "    evaluate_method(mc_confidence_sets, f'mc_')\n",
        "\n",
        "  for key in results.keys():\n",
        "    results[key] = jnp.array(results[key])\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE2QyVG6T_xu"
      },
      "outputs": [],
      "source": [
        "def plot_label_randomness(predictions, data, **kwargs):\n",
        "  alpha = 0.05\n",
        "  results = run_trial(predictions, data, alpha, seed=kwargs.get('seed', 0))\n",
        "  vmax = 0\n",
        "  hist, _ = plot_hist(\n",
        "      jnp.mean(results['mc_aggregated_coverages'], axis=-1), normalize=True,\n",
        "      alpha=0.65, label='Aggregated coverage')\n",
        "  vmax = max(vmax, jnp.max(hist))\n",
        "  plt.vlines(1 - alpha, 0, vmax, color='black', label='Target')\n",
        "  plt.title('Variation in aggregated coverage from sampled labels')\n",
        "  plt.xlabel('Empirical coverage')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend()\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  if kwargs.get('name', False):\n",
        "      plt.savefig(kwargs.get('name') + '.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc5wV7m3T_xv"
      },
      "outputs": [],
      "source": [
        "plot_label_randomness(predictions, data, seed=3, name=f'label_randomness3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5IpwajOT_xv"
      },
      "outputs": [],
      "source": [
        "def run_trial(\n",
        "    predictions, data, alpha=0.05, num_trials=100, split=0.5):\n",
        "  \"\"\"Run a conformal prediction experiment.\"\"\"\n",
        "  results = {}\n",
        "  ms = [1, 5, 10]\n",
        "  metrics = [\n",
        "      'inefficiencies',\n",
        "      'true_coverages',\n",
        "      'aggregated_coverages',\n",
        "      'top1_coverages',\n",
        "  ]\n",
        "  methods = ['true_', 'top1_'] + [f'mc{m}_' for m in ms]\n",
        "  for method in methods:\n",
        "    for metric in metrics:\n",
        "      results[method + metric] = []\n",
        "  keys = jax.random.split(jax.random.PRNGKey(0), 2 * num_trials)\n",
        "  for t in range(num_trials):\n",
        "    permutation = jax.random.permutation(keys[2 * t], predictions.shape[0])\n",
        "    val_examples = int(predictions.shape[0]*split)\n",
        "    val_predictions = predictions[permutation[:val_examples]]\n",
        "    test_predictions = predictions[permutation[val_examples:]]\n",
        "    val_human_ground_truth = data['test_smooth_labels'][permutation[:val_examples]]\n",
        "    test_human_ground_truth = data['test_smooth_labels'][permutation[val_examples:]]\n",
        "    val_ground_truth = data['test_labels'][permutation[:val_examples]]\n",
        "    test_ground_truth = data['test_labels'][permutation[val_examples:]]\n",
        "\n",
        "    def evaluate_method(confidence_sets, key):\n",
        "      results[f'{key}inefficiencies'].append(classification_metrics.size(\n",
        "          confidence_sets))\n",
        "      test_one_hot_ground_truth = jax.nn.one_hot(\n",
        "          test_ground_truth, confidence_sets.shape[1])\n",
        "      results[f'{key}true_coverages'].append(classification_metrics.aggregated_coverage(\n",
        "          confidence_sets, test_one_hot_ground_truth))\n",
        "      test_top1_ground_truth = jax.nn.one_hot(\n",
        "          jnp.argmax(test_human_ground_truth, axis=1),\n",
        "          test_human_ground_truth.shape[1])\n",
        "      results[f'{key}top1_coverages'].append(\n",
        "          classification_metrics.aggregated_coverage(\n",
        "              confidence_sets, test_top1_ground_truth))\n",
        "      results[f'{key}aggregated_coverages'].append(\n",
        "          classification_metrics.aggregated_coverage(\n",
        "              confidence_sets, test_human_ground_truth))\n",
        "\n",
        "    val_true_labels = val_ground_truth\n",
        "    val_top1_labels = jnp.argmax(val_human_ground_truth, axis=1)\n",
        "    true_threshold = conformal_prediction.calibrate_threshold(\n",
        "        val_predictions, val_true_labels, alpha)\n",
        "    top1_threshold = conformal_prediction.calibrate_threshold(\n",
        "        val_predictions, val_top1_labels, alpha)\n",
        "    true_confidence_sets = conformal_prediction.predict_threshold(\n",
        "          test_predictions, true_threshold)\n",
        "    top1_confidence_sets = conformal_prediction.predict_threshold(\n",
        "          test_predictions, top1_threshold)\n",
        "    evaluate_method(true_confidence_sets, 'true_')\n",
        "    evaluate_method(top1_confidence_sets, 'top1_')\n",
        "\n",
        "    for m in ms:\n",
        "      val_mc_predictions, mc_labels = monte_carlo.sample_mc_labels(\n",
        "        keys[2 * t + 1], val_predictions, val_human_ground_truth, num_samples=m)\n",
        "      val_mc_predictions = val_mc_predictions.reshape(-1, val_mc_predictions.shape[-1])\n",
        "      val_mc_labels = mc_labels.reshape(-1)\n",
        "      mc_threshold = conformal_prediction.calibrate_threshold(\n",
        "          val_mc_predictions, val_mc_labels, alpha)\n",
        "      mc_confidence_sets = conformal_prediction.predict_threshold(\n",
        "          test_predictions, mc_threshold)\n",
        "      evaluate_method(mc_confidence_sets, f'mc{m}_')\n",
        "\n",
        "  for key in results.keys():\n",
        "    results[key] = jnp.array(results[key])\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaV71KfTT_xw"
      },
      "outputs": [],
      "source": [
        "def plot_std(alpha=0.05, num_trials=100, ms=[1, 5, 10], **kwargs):\n",
        "  \"\"\"Plot standard deviation for MC conformal prediction.\"\"\"\n",
        "  results = []\n",
        "  splits = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "  for split in splits:\n",
        "    results.append(run_trial(\n",
        "        predictions, data, alpha=0.05, num_trials=num_trials, split=split))\n",
        "\n",
        "  for m in ms:\n",
        "    values = [result[f'mc{m}_aggregated_coverages'] for result in results]\n",
        "    plt.plot(\n",
        "        splits,\n",
        "         [jnp.std(jnp.mean(value, axis=-1), axis=-1) for value in values],\n",
        "        label=r\"$m =$\" + f'{m} sampled labels')\n",
        "  plt.title('Standard deviation in aggregated coverage')\n",
        "  plt.xlabel('Fraction of calibration data')\n",
        "  plt.ylabel('Std in empirical coverage')\n",
        "  plt.legend(bbox_to_anchor=[1, 1], loc='upper right')\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  plt.savefig('mc_std_aggregated_lines_wo.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()\n",
        "\n",
        "  vmax = 0\n",
        "  for m in ms:\n",
        "    hist, _ = plot_hist(\n",
        "        jnp.mean(results[0][f'mc{m}_aggregated_coverages'], axis=-1), normalize=True,\n",
        "        alpha=0.65, label=r\"$m =$\" + f'{m} sampled labels', bins=40, range=(0.92, 0.98))\n",
        "    vmax = max(vmax, jnp.max(hist))\n",
        "  plt.vlines(1 - alpha, 0, vmax, color='black', label='Target')\n",
        "  plt.title('Aggregated coverage for 10% calibration data')\n",
        "  plt.xlabel('Empirical coverage')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.xlim(0.9, 0.98)\n",
        "  plt.legend(bbox_to_anchor=[0, 1], loc='upper left')\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  plt.savefig('mc_std_aggregated_histogram_wo.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBhfN7QaT_xw"
      },
      "outputs": [],
      "source": [
        "plot_std(alpha=0.05, num_trials=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dE_EnXeT_xw"
      },
      "source": [
        "## ECDF-corrected Monte Carlo conformal prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os_6Bo2DT_xx"
      },
      "outputs": [],
      "source": [
        "def plot_ecdf(**kwargs):\n",
        "  num_tests = 10\n",
        "  num_examples = 10000\n",
        "  val_examples = 10000//2\n",
        "  all_p_values = jax.random.uniform(jax.random.PRNGKey(0), (num_tests, num_examples))\n",
        "  dependent_p_values = all_p_values\n",
        "  all_p_values = jnp.concatenate((all_p_values, dependent_p_values), axis=0)\n",
        "  combined_p_values = jnp.mean(all_p_values, axis=0)\n",
        "  val_combined_p_values = combined_p_values[:val_examples]\n",
        "  test_combined_p_values = combined_p_values[val_examples:]\n",
        "  test_corrected_p_values = p_value_combination.combine_ecdf_p_values(\n",
        "      val_combined_p_values, test_combined_p_values)\n",
        "\n",
        "  plot_hist(\n",
        "      test_combined_p_values, normalize=True,\n",
        "       alpha=0.65, label='Averaged p-values')\n",
        "  plot_hist(\n",
        "      test_corrected_p_values, normalize=True,\n",
        "      alpha=0.65, label=f'ECDF corrected p-values')\n",
        "  plt.legend()\n",
        "  plt.title(f'Distribution of combined p-values')\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  plt.savefig('ecdf_p_values.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()\n",
        "\n",
        "  baseline_coverages = []\n",
        "  method_coverages = []\n",
        "  max_method_coverages = []\n",
        "  min_method_coverages = []\n",
        "  delta = 0.0001\n",
        "  alphas = jnp.linspace(0, 1, 26)\n",
        "  epsilon = np.sqrt(np.log(2. / delta) / (2 * val_examples))\n",
        "  for alpha in alphas:\n",
        "    baseline_coverages.append(jnp.mean(test_combined_p_values \u003e= alpha))\n",
        "    method_coverage = jnp.mean(test_corrected_p_values \u003e= alpha)\n",
        "    method_coverages.append(method_coverage)\n",
        "    max_method_coverages.append(min(1, method_coverage + epsilon))\n",
        "    min_method_coverages.append(max(0, method_coverage - epsilon))\n",
        "  max_method_coverages = np.array(max_method_coverages)\n",
        "  min_method_coverages = np.array(min_method_coverages)\n",
        "\n",
        "  plt.plot(1 - alphas, baseline_coverages, label='Baseline')\n",
        "  plt.plot(1 - alphas, method_coverages, label=f'ECDF', color='green')\n",
        "  plt.plot(1 - alphas, max_method_coverages, color='green', alpha=0.2)\n",
        "  plt.plot(1 - alphas, min_method_coverages, color='green', alpha=0.2)\n",
        "  plt.fill_between(\n",
        "      alphas, 1 - max_method_coverages, 1 - min_method_coverages, alpha=0.1,\n",
        "      color='green', label='ECDF $1 - \\delta$ band')\n",
        "  plt.title(f'ECDF correction of p-values')\n",
        "  plt.xlabel(r'Target coverage $1 - \\alpha$')\n",
        "  plt.ylabel('Empirical coverage')\n",
        "  plt.legend()\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  plt.savefig('ecdf_lines.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMlnlOFaT_xx"
      },
      "outputs": [],
      "source": [
        "plot_ecdf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxSQENifT_xy"
      },
      "outputs": [],
      "source": [
        "def plot_coverage_ecdf_mccp(\n",
        "    predictions, data,\n",
        "    num_samples=10, split=0.5, alpha=0.05, num_trials=10, **kwargs):\n",
        "  \"\"\"Plot coverage for ECDF based MC conformal prediction.\"\"\"\n",
        "  coverages = []\n",
        "  mc_coverages = []\n",
        "  corrected_mc_coverages = []\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "  for _ in range(num_trials):\n",
        "    permutation_rng, mc_rng, rng = jax.random.split(rng, 3)\n",
        "    permutation = jax.random.permutation(permutation_rng, predictions.shape[0])\n",
        "    _, num_classes = predictions.shape\n",
        "    val_examples = int(predictions.shape[0]*split)\n",
        "    val_predictions = predictions[permutation[:val_examples]]\n",
        "    test_predictions = predictions[permutation[val_examples:]]\n",
        "    val_human_ground_truth = data['test_smooth_labels'][permutation[:val_examples]]\n",
        "    val_labels = data['test_labels'][permutation[:val_examples]]\n",
        "    test_labels = data['test_labels'][permutation[val_examples:]]\n",
        "\n",
        "    p_values = conformal_prediction.compute_p_values(\n",
        "        val_predictions, val_labels, test_predictions)\n",
        "\n",
        "    mc_p_values = monte_carlo.compute_mc_p_values(\n",
        "        mc_rng, val_predictions,\n",
        "        val_human_ground_truth, test_predictions, num_samples)\n",
        "    mc_p_values = jnp.mean(mc_p_values, axis=0)\n",
        "\n",
        "    corrected_mc_p_values = monte_carlo.compute_mc_ecdf_p_values(\n",
        "        mc_rng, val_predictions,\n",
        "        val_human_ground_truth, test_predictions, num_samples)\n",
        "\n",
        "    confidence_sets = conformal_prediction.predict_p_values(p_values, alpha)\n",
        "    mc_confidence_sets = conformal_prediction.predict_p_values(mc_p_values, alpha)\n",
        "    corrected_mc_confidence_sets = conformal_prediction.predict_p_values(\n",
        "        corrected_mc_p_values, alpha)\n",
        "\n",
        "    coverages.append(classification_metrics.aggregated_coverage(\n",
        "        confidence_sets, jax.nn.one_hot(test_labels, num_classes)))\n",
        "    mc_coverages.append(classification_metrics.aggregated_coverage(\n",
        "        mc_confidence_sets, jax.nn.one_hot(test_labels, num_classes)))\n",
        "    corrected_mc_coverages.append(classification_metrics.aggregated_coverage(\n",
        "        corrected_mc_confidence_sets,\n",
        "        jax.nn.one_hot(test_labels, num_classes)))\n",
        "\n",
        "  mc_coverages = jnp.array(mc_coverages)\n",
        "  corrected_mc_coverages = jnp.array(corrected_mc_coverages)\n",
        "\n",
        "  vmax = 0\n",
        "  hist, _ = colab_utils.plot_hist(\n",
        "      jnp.mean(mc_coverages, axis=1),\n",
        "      normalize=True, alpha=0.65, label='Monte Carlo CP')\n",
        "  vmax = max(np.max(hist), vmax)\n",
        "  hist, _ = colab_utils.plot_hist(\n",
        "      jnp.mean(corrected_mc_coverages, axis=1),\n",
        "      normalize=True, alpha=0.65, label='ECDF Monte Carlo CP')\n",
        "  vmax = max(np.max(hist), vmax)\n",
        "  plt.vlines(1 - alpha, 0, vmax, label='Target', color='black')\n",
        "  plt.title('Aggregated coverage of ECDF-based approach')\n",
        "  plt.xlabel('Empirical coverage')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.xticks([0.94, 0.95, 0.96])\n",
        "  plt.legend()\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  if kwargs.get('name', False):\n",
        "      plt.savefig(kwargs.get('name') + '.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS_0TTZQT_xz"
      },
      "outputs": [],
      "source": [
        "plot_coverage_ecdf_mccp(\n",
        "    predictions, data, alpha=0.05, num_samples=10, num_trials=100,\n",
        "    name='ecdf_coverage')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbE94I6IT_xz"
      },
      "source": [
        "## Aggregated conformity scores\n",
        "\n",
        "This corresponds to an experiment calibrating with so-called aggregated conformity scores (in the first version our paper called *expected* conformity scores) which leads to plausibility regions.\n",
        "\n",
        "Please see the first version of our paper on ArXiv: [arxiv.org/abs/2307.09302v1](https://arxiv.org/abs/2307.09302v1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo8Q-hJ6T_xz"
      },
      "outputs": [],
      "source": [
        "def plot_conformity_scores(predictions, data, **kwargs):\n",
        "  num_samples = 10\n",
        "  labels = data['test_labels']\n",
        "  smooth_labels = data['test_smooth_labels']\n",
        "  num_examples = predictions.shape[0]\n",
        "  true_scores = predictions[jnp.arange(num_examples), labels]\n",
        "  top1_scores = predictions[\n",
        "      jnp.arange(num_examples), jnp.argmax(smooth_labels, axis=1)]\n",
        "  aggregated_scores = jnp.sum(predictions * smooth_labels, axis=1)\n",
        "\n",
        "  plot_hist(\n",
        "      true_scores, label='True scores $E(x,y)$', range=(0, 1), bins=50,\n",
        "      normalize=True, alpha=0.6)\n",
        "  plot_hist(\n",
        "      top1_scores, label='Voted scores $E(x,argmax_k\\lambda_k)$', range=(0, 1), bins=50,\n",
        "      normalize=True, alpha=0.6)\n",
        "  plot_hist(\n",
        "      aggregated_scores, label=r\"Aggregated scores $e(x,\\lambda)$\",\n",
        "      range=(0, 1), bins=50, normalize=True, alpha=0.6)\n",
        "  plt.legend()\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.xlabel('Conformity score')\n",
        "  plt.title('Conformity score histograms')\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  plt.savefig('conformity_scores.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(\n",
        "      jnp.arange(num_examples) / num_examples,\n",
        "      jnp.sort(true_scores),\n",
        "      label='True scores $E(x,y)$')\n",
        "  plt.plot(\n",
        "      jnp.arange(num_examples) / num_examples, jnp.sort(top1_scores),\n",
        "      label='Voted scores $E(x,argmax_k\\lambda_k)$')\n",
        "  plt.plot(\n",
        "      jnp.arange(num_examples) / num_examples, jnp.sort(aggregated_scores),\n",
        "      label='Aggregated scores $e(x,\\lambda)$')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Frequency')\n",
        "  plt.ylabel('Conformity score')\n",
        "  plt.title('Conformity score CDFs')\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  plt.savefig('conformity_scores_cdf.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9A6OEakT_x0"
      },
      "outputs": [],
      "source": [
        "plot_conformity_scores(predictions, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UevthQB_T_x0"
      },
      "source": [
        "### Reduced plausibility regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZaOyHJ_T_x0"
      },
      "outputs": [],
      "source": [
        "def visualize_confidence_regions(\n",
        "    rng, predictions, data, indices, alpha=0.05, split=0.5, **kwargs):\n",
        "  \"\"\"Visualize confidence regions.\"\"\"\n",
        "  permutation = jax.random.permutation(rng, predictions.shape[0])\n",
        "  val_examples = int(predictions.shape[0]*split)\n",
        "  num_classes = predictions.shape[1]\n",
        "  val_predictions = predictions[permutation[:val_examples]]\n",
        "  test_predictions = predictions[permutation[val_examples:]]\n",
        "  val_human_ground_truth = data['test_smooth_labels'][permutation[:val_examples]]\n",
        "  test_human_ground_truth = data['test_smooth_labels'][permutation[val_examples:]]\n",
        "  val_ground_truth = data['test_labels'][permutation[:val_examples]]\n",
        "  test_ground_truth = data['test_labels'][permutation[val_examples:]]\n",
        "  test_examples = data['test_examples'][permutation[val_examples:]]\n",
        "\n",
        "  baseline_threshold = conformal_prediction.calibrate_threshold(\n",
        "      val_predictions, jnp.argmax(val_human_ground_truth, axis=1), alpha)\n",
        "  baseline_confidence_sets = conformal_prediction.predict_threshold(\n",
        "      test_predictions, baseline_threshold)\n",
        "\n",
        "  threshold = plausibility_regions.calibrate_plausibility_regions(\n",
        "      val_predictions, val_human_ground_truth, alpha)\n",
        "  distributions, coverages = plausibility_regions.predict_plausibility_regions(\n",
        "      test_predictions, threshold, num_grid_points=50)\n",
        "  k = 1\n",
        "  confidence_sets = plausibility_regions.reduce_plausibilities_to_topk(\n",
        "      distributions, coverages, k=k)\n",
        "\n",
        "  colors = np.array([\n",
        "      [228,26,28],\n",
        "      [55,126,184],\n",
        "      [77,175,74],\n",
        "  ]) / 255.\n",
        "  colab_utils.plot_smooth_data(\n",
        "      data['train_examples'], data['train_smooth_labels'],\n",
        "      highlight_points=test_examples[indices], boundary=True,\n",
        "      name='data_smooth_marked', colors=colors)\n",
        "  cmap = matplotlib.cm.get_cmap('viridis')\n",
        "  plt.text(\n",
        "      0.475, 0.8, 'MLP class. boundary', color=cmap(0), fontdict={'fontsize': 14})\n",
        "  plt.show()\n",
        "\n",
        "  projected_distributions = colab_utils.project_simplex(distributions)\n",
        "  for i, n in enumerate(indices):\n",
        "    print('Case:', n)\n",
        "    print('Human ground truth:', test_human_ground_truth[n])\n",
        "    print('Conformity scores:', test_predictions[n])\n",
        "    print('Baseline confidence sets:', baseline_confidence_sets[n])\n",
        "    print(f'Top-{k} confidence set:', confidence_sets[n])\n",
        "\n",
        "    plt.bar(\n",
        "        jnp.arange(num_classes),\n",
        "        test_human_ground_truth[n],\n",
        "        label=r\"Plausibility $\\lambda_k = p(y=k|x)$\", alpha=0.65)\n",
        "    plt.bar(\n",
        "        jnp.arange(num_classes),\n",
        "        test_predictions[n],\n",
        "        label=r\"Conf. scores $E(x, k) = \\pi_k(x)$\", alpha=0.65)\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.legend(loc='lower left', bbox_to_anchor=(-0.5, 1))\n",
        "    plt.gcf().set_size_inches(2, 1)\n",
        "    plt.xticks([0, 1, 2])\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'data_smooth_{i + 1}.pdf', bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "    colab_utils.plot_simplex(projected_distributions, coverages[n])\n",
        "    plt.savefig(f'data_smooth_{i + 1}1.png', bbox_inches=\"tight\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFDUvyxiT_x1"
      },
      "outputs": [],
      "source": [
        "visualize_confidence_regions(\n",
        "    jax.random.PRNGKey(0), predictions, data,\n",
        "    indices=np.array([10, 6, 12, 20]), alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwJdaqH-T_x1"
      },
      "outputs": [],
      "source": [
        "def run_reduction_trials(\n",
        "    rng, predictions, data, alpha=0.05, trials=100, split=0.5):\n",
        "  \"\"\"Visualize confidence regions.\"\"\"\n",
        "  results = {}\n",
        "  tags = ['true_coverages', 'plausibility_coverages']\n",
        "  for tag in tags:\n",
        "    results[tag] = []\n",
        "  keys = jax.random.split(rng, trials)\n",
        "  for key in keys:\n",
        "    permutation = jax.random.permutation(key, predictions.shape[0])\n",
        "    val_examples = int(predictions.shape[0]*split)\n",
        "    val_predictions = predictions[permutation[:val_examples]]\n",
        "    test_predictions = predictions[permutation[val_examples:]]\n",
        "    val_human_ground_truth = data['test_smooth_labels'][\n",
        "        permutation[:val_examples]]\n",
        "    test_human_ground_truth = data['test_smooth_labels'][\n",
        "        permutation[val_examples:]]\n",
        "    test_ground_truth = data['test_labels'][permutation[val_examples:]]\n",
        "\n",
        "    threshold = plausibility_regions.calibrate_plausibility_regions(\n",
        "        val_predictions, val_human_ground_truth, alpha)\n",
        "    distributions, coverages = plausibility_regions.predict_plausibility_regions(\n",
        "        test_predictions, threshold)\n",
        "    confidence_sets = plausibility_regions.reduce_plausibilities_to_topk(\n",
        "        distributions, coverages, k=1)\n",
        "    num_classes = test_predictions.shape[1]\n",
        "    true_coverages = classification_metrics.aggregated_coverage(\n",
        "        confidence_sets, jax.nn.one_hot(test_ground_truth, num_classes))\n",
        "    plausibility_coverages = plausibility_regions.check_plausibility_regions(\n",
        "        test_predictions, test_human_ground_truth, threshold)\n",
        "    results['true_coverages'].append(true_coverages)\n",
        "    results['plausibility_coverages'].append(plausibility_coverages)\n",
        "  for tag in tags:\n",
        "    results[tag] = jnp.array(results[tag])\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOeWrUNZT_x2"
      },
      "outputs": [],
      "source": [
        "def plot_reduced_plausibility_regions(predictions, data, alpha=0.05, **kwargs):\n",
        "  plausibility_results = run_reduction_trials(\n",
        "      jax.random.PRNGKey(0), predictions, data, alpha=alpha)\n",
        "\n",
        "  hist, _ = plot_hist(\n",
        "      jnp.mean(plausibility_results['plausibility_coverages'], axis=-1),\n",
        "      normalize=True, label='Plausibility coverage')\n",
        "  vmax = np.max(hist)\n",
        "  hist, _ = plot_hist(\n",
        "      jnp.mean(plausibility_results['true_coverages'], axis=-1),\n",
        "      normalize=True, label='True label coverage')\n",
        "  vmax = max(vmax, np.max(hist))\n",
        "  plt.vlines(0.95, 0, vmax, color='red', label='Target plausibility coverage')\n",
        "  plt.xlabel('Empirical coverage')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend()\n",
        "  plt.title('Coverage of reduced plausibility regions')\n",
        "  plt.gcf().set_size_inches(kwargs.get('width', 5), kwargs.get('height', 3))\n",
        "  plt.savefig('reduced_plausibility_regions.pdf', bbox_inches=\"tight\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmHUqH1rT_x2"
      },
      "outputs": [],
      "source": [
        "plot_reduced_plausibility_regions(predictions, data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
